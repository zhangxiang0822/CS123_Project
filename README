## University of Chicago | Spring 2019

Repository for CMSC 12300 (Computer Science with Applications-3) Group Project

This readMe is structured as following:
I. PROJECT GOAL
II. CODE STRUCTURE
III. HOW TO RUN THE CODE
IV. TO DO LIST


## I. PROJECT GOAL

We try to understand the patent citation pattern across US industries from 1970 up to date. This includes how the network strucuture of
industry-to-industry patent citation has been shifting over time. What does the intra-industry vs cross-industry patent citation look like.
And how to find the shortest path between certain patent-citation pair in a large network structure.

Our project highlights two techniques: 
  - First is merging large data using MapReduce and its implementation on Google Cloud plateform
    Due to the difference in infrastructure/configuration between Google cloud service (GCS) and local machine. We debug the discrepancy 
    between these two scenarios and modify our code to be compatible with cloud computing.
  - Second important feature is to perform a parallel version (base on mapreduce) shortest-path algorithm (Dijkstra)
    The main challenge of this task is to figure out a way to parallelize local-machine-based Dijsktra algorithm and modify it to be
    compatible with MapReduce job structure and big data computing on GCS.

## II. CODE STRUCTURE


All code processing results are put under "code" folder. They are listed below
  # Data Processing
  - data_preprocessing.py:  This is the code merging "nber data" with "nber_subcategory dataset" on Google Cloud
  - data preprocessing2.py: This is the code merging output of "data_preprocessing.py" to citation dataset on Google Cloud
  - data_preprocessing_smalldata.py: This is the code preparing data for running Dijkstra algorithm on small data

  # Shortest Path
  - Dijkstra_smalldata.py:  This is the code implementing Dijkstra algorithm on small data set (using sample input data)
  - Dijkstra_parallel.py:   This is the code implementing Dijkstra algorithm (in fact, breadth-first search) on Google Cloud
  
  # Summary Statistics
  - count_citation_byindustry
  - count_citation_byyear
  - count_citation_flow

## III. HOW TO RUN THE CODE
Suggestion: please try running it on local first!

  i. first-step merging
      python data_preprocessing.py -r dataproc ../data/nber_subcategory.tsv ../data/nber_sample.tsv --runner_type="GCS" --merger_type="left" > ../data/data_merge1.json
  ii. second-step merging (must be run only after first merging is complete! Due to oversize data, we only put data_merge1_small.json in the dir)
      python data_preprocessing2.py -r dataproc ../data/sample_data.tsv ../data/data_merge1.json --runner_type="GCS" --merger_type="left" > ../data/data_merge2.json
  iii. format data before Dijsktra
      python Dijkstra_parallel.py ../data/sample_data_formated.csv --start_point="1" --iteration=3 > ../data/shortest_path_data.json
  iv. run summary statistics and interpret output
     (to be finished)

## IV. New update
  i. We finished running all the merging code on GCS and obtained correct results
  ii. Due to time limit, we only test the parallelized Dijsktra algorithm on a relatively small scaled data
  iii. We generate the results: patent citation pattern across industry and a graphic illustration of shortest-path detection algorithm.
       (please see the results section of the final report)
  iv. we didn't change much of the code, but focusing more on the implementation of algorithm.
  
# Contributors
Anhua Chen [ChenAnHua](https://github.com/ChenAnhua)

David Liu [dliu5](https://github.com/dliu5)

Xiang Zhang [zhangxiang0822](https://github.com/zhangxiang0822)

Xiuyuan Zhang [xiuyuanzhang](https://github.com/xiuyuanzhang)
